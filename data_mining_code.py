# -*- coding: utf-8 -*-
"""Data_Mining_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MD8PTdM2EDBY1i_ZBBzfFWHcmYE3_14M

# PROBAT'S QUALITY EVALUATION & COST BENEFIT ANALYSIS MODEL ðŸ“Š

**IMPORTING KAGGLE FILE**
"""

#Install the kagglehub package quietly (-q suppresses output)
!pip install -q kagglehub

#Import the kagglehub library to use its functions for dataset management
import kagglehub

# Download latest version
path = kagglehub.dataset_download("podsyp/production-quality")

#Print the path to the downloaded dataset files for verification and future reference
print("Path to dataset files:", path)

# move files /root/.cache/kagglehub/datasets/podsyp/production-quality/versions/1 to /content/
#Prepare to move the downloaded dataset files from the cache directory to a specified content directory

import shutil # Import shutil for file operations like moving files
import os # Import os for interacting with the operating system

source_dir = '/root/.cache/kagglehub/datasets/podsyp/production-quality/versions/1' # Define the source directory where the dataset is initially stored after download
destination_dir = '/content/' # Define the destination directory where the files will be moved for easier access

if os.path.exists(source_dir): # Check if the source directory exists to avoid errors during file moving
  try:
    shutil.move(source_dir, destination_dir)
    print(f"Successfully moved files from '{source_dir}' to '{destination_dir}'")
  except Exception as e:
    print(f"Error moving files: {e}")
else:
  print(f"Source directory '{source_dir}' does not exist.")

pip install umap-learn #Install the umap-learn package, which is used for dimensionality reduction and visualization at later stage

"""**DATA PREPARATION**"""

#Importing all necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import AdaBoostClassifier
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns
import pprint
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import umap
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# here we Load the data
data_x = pd.read_csv('/content/1/data_X.csv')
data_y = pd.read_csv('/content/1/data_Y.csv')

#checking all the columns of x.csv
data_x.columns

#checking all the columns of y.csv
data_y.columns

#using .info to check Dtypes for x file
data_x.info()

# using .info to check Dtypes for x file
data_y.info()

# Convert to datetime first (if not already in datetime format)
data_x['date_time'] = pd.to_datetime(data_x['date_time'])
data_y['date_time'] = pd.to_datetime(data_y['date_time'])

# Convert the datetime to Unix timestamp
data_x['date_time'] = data_x['date_time'].apply(lambda x: x.timestamp())
data_y['date_time'] = data_y['date_time'].apply(lambda x: x.timestamp())

print(data_x.head()) # Display the first few rows of the 'data_x' DataFrame to check its structure and contents
print(data_y.head()) # Display the first few rows of the 'data_y' DataFrame to check its structure and contents

# Merge the 'data_x' and 'data_y' DataFrames on the 'date_time' column using an inner join
# This will combine rows where the 'date_time' values match in both DataFrames
data_z =pd.merge(data_x, data_y, on='date_time',how='inner')

# Print the names of the columns in the merged DataFrame 'data_z' to understand the structure of the combined data
print(data_z.columns)

data_z= data_z.drop_duplicates() # droping duplicates
data_z = data_z.dropna() # droping null values

# remove time stamp column
data_z = data_z.drop(columns=['date_time'])

data_z.head() #Displaying the first few rows of the data

# Normalize numerical features
numerical_features =data_z.columns.difference(['quality'])
print(numerical_features)
scaler = StandardScaler() #to treat every cloloum equal to get values between 0-1 we do scaling
data_z[numerical_features] = scaler.fit_transform(data_z[numerical_features])

# if quality variable >= 410 then quality= 1

data_z['quality'] = data_z['quality'].apply(lambda x: 1 if x >= 410 else 0)

data_z['quality'].value_counts()

# Separate features and target variable
X = data_z.drop(columns=['quality'])
y = data_z['quality']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**DATA MODELING & EVALUATION**"""

# Random Forest Classifier


# Random Forest pipeline setup
rf_pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('rf_classifier', RandomForestClassifier(random_state=42))
])

# Hyperparameter tuning for Random Forest
rf_params = {
    'rf_classifier__n_estimators': [50, 100, 150],
    'rf_classifier__max_depth': [10, 20, None]
}

# Grid Search with 5-fold Cross-Validation
rf_grid = GridSearchCV(rf_pipeline, rf_params, cv=5, scoring='f1')
rf_grid.fit(X_train, y_train)

# Best Random Forest model and predictions
best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test)

print("Random Forest - Best Parameters:", rf_grid.best_params_)
print("Random Forest - Classification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_rf)

# Visualization of Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix for Random Forest Classifier')
plt.show()

# AdaBoost Classifier

# AdaBoost pipeline setup
ada_pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('ada_classifier', AdaBoostClassifier(random_state=42))
])

# Hyperparameter tuning for AdaBoost
ada_params = {
    'ada_classifier__n_estimators': [50, 100, 150],
    'ada_classifier__learning_rate': [0.5, 1.0, 1.5]
}

# Grid Search with 5-fold Cross-Validation
ada_grid = GridSearchCV(ada_pipeline, ada_params, cv=5, scoring='f1')
ada_grid.fit(X_train, y_train)

# Best AdaBoost model and predictions
best_ada = ada_grid.best_estimator_
y_pred_ada = best_ada.predict(X_test)

print("AdaBoost - Best Parameters:", ada_grid.best_params_)
print("AdaBoost - Classification Report:\n", classification_report(y_test, y_pred_ada))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_ada)

# Visualization of Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix for Adaboost Classifier')
plt.show()

# Support Vector Classifier (SVC)
# SVC pipeline setup
svc_pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('svc_classifier', SVC(random_state=42))
])

# Hyperparameter tuning for SVC
svc_params = {
    'svc_classifier__C': [0.1, 1, 10],
    'svc_classifier__kernel': ['linear', 'rbf']
}

# Grid Search with 5-fold Cross-Validation
svc_grid = GridSearchCV(svc_pipeline, svc_params, cv=5, scoring='f1')
svc_grid.fit(X_train, y_train)

# Best SVC model and predictions
best_svc = svc_grid.best_estimator_
y_pred_svc = best_svc.predict(X_test)

print("SVC - Best Parameters:", svc_grid.best_params_)
print("SVC - Classification Report:\n", classification_report(y_test, y_pred_svc))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_svc)

# Visualization of Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix for SVM Classifier')
plt.show()

# Logistic Regression (SGDClassifier) pipeline setup
logreg_pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('logreg_classifier', SGDClassifier(random_state=42, loss='log_loss'))
])

# Hyperparameter tuning for Logistic Regression
logreg_params = {
    'logreg_classifier__alpha': [0.0001, 0.001, 0.01],
    'logreg_classifier__l1_ratio': [0.0, 0.5, 1.0],
    'logreg_classifier__penalty': ['l2', 'elasticnet']
}

# Grid Search with 5-fold Cross-Validation
logreg_grid = GridSearchCV(logreg_pipeline, logreg_params, cv=5, scoring='f1')
logreg_grid.fit(X_train, y_train)

# Best Logistic Regression model and predictions
best_logreg = logreg_grid.best_estimator_
y_pred_logreg = best_logreg.predict(X_test)

print("Logistic Regression - Best Parameters:", logreg_grid.best_params_)
print("Logistic Regression - Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_logreg)

# Visualization of Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix for  Logistic Regression')
plt.show()

# Comparison of Models with visualization

print("F1 Scores Comparison:")
print("Random Forest F1 Score:", f1_score(y_test, y_pred_rf))
print("AdaBoost F1 Score:", f1_score(y_test, y_pred_ada))
print("SVC F1 Score:", f1_score(y_test, y_pred_svc))
print("Logistic Regression F1 Score:", f1_score(y_test, y_pred_logreg))

# F1 scores from your model evaluation
model_names = ['Random Forest', 'AdaBoost', 'SVC', 'Logistic Regression']
f1_scores = [0.9174, 0.9036, 0.9022, 0.8953]  # F1 scores rounded from your example

# Pastel color palette
colors = ['#A3C1DA', '#FFB6C1', '#C3E6CB', '#FDD7E4']  # Light pastel shades for bars

# Plotting the bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, f1_scores, color=colors)

# Adding the F1 score values on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 4), ha='center', va='bottom')

# Labels and Title
plt.xlabel("Model")
plt.ylabel("F1 Score")
plt.title("F1 Score Comparison of Different Models")
plt.ylim(0.85, 1.0)  # Adjust y-axis for better visibility

# Show plot
plt.show()

from sklearn.metrics import classification_report, f1_score

# Dictionary to store F1-scores for each model
f1_scores = {
    'Random Forest': {},
    'AdaBoost': {},
    'SVC': {},
    'Logistic Regression': {}
}

# Random Forest
report_rf = classification_report(y_test, y_pred_rf, output_dict=True)
f1_scores['Random Forest']['macro'] = report_rf['macro avg']['f1-score']
f1_scores['Random Forest']['weighted'] = report_rf['weighted avg']['f1-score']

# AdaBoost
report_ada = classification_report(y_test, y_pred_ada, output_dict=True)
f1_scores['AdaBoost']['macro'] = report_ada['macro avg']['f1-score']
f1_scores['AdaBoost']['weighted'] = report_ada['weighted avg']['f1-score']

# SVC
report_svc = classification_report(y_test, y_pred_svc, output_dict=True)
f1_scores['SVC']['macro'] = report_svc['macro avg']['f1-score']
f1_scores['SVC']['weighted'] = report_svc['weighted avg']['f1-score']

# Logistic Regression
report_logreg = classification_report(y_test, y_pred_logreg, output_dict=True)
f1_scores['Logistic Regression']['macro'] = report_logreg['macro avg']['f1-score']
f1_scores['Logistic Regression']['weighted'] = report_logreg['weighted avg']['f1-score']

# Display F1 scores
# Iterate through the f1_scores dictionary and print the macro and weighted F1 scores for each model
for model, scores in f1_scores.items():
    print(f"{model} - Macro F1 Score:", scores['macro'])
    print(f"{model} - Weighted F1 Score:", scores['weighted'])

# Feature Importance Extraction (Random Forest and AdaBoost)
# Feature importance from Random Forest
rf_importances = best_rf.named_steps['rf_classifier'].feature_importances_
rf_feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': rf_importances}).sort_values(by='Importance', ascending=False)
print("Random Forest Feature Importances:\n", rf_feature_importance_df)

# Feature importance from AdaBoost
ada_importances = best_ada.named_steps['ada_classifier'].feature_importances_
ada_feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': ada_importances}).sort_values(by='Importance', ascending=False)
print("AdaBoost Feature Importances:\n", ada_feature_importance_df)

"""**COST BENEFIT ANALYSIS**"""

# Assumed costs
false_positive_cost = 5000  # Cost of a false positive (Low quality classified as High quality)
false_negative_cost = 500   # Cost of a false negative (High quality classified as Low quality)

def calculate_costs(y_true, y_pred, fp_cost, fn_cost):
    """
    Calculate the total cost based on false positives and false negatives.

    Parameters:
    - y_true: Ground truth labels.
    - y_pred: Predicted labels.
    - fp_cost: Cost per false positive.
    - fn_cost: Cost per false negative.

    Returns:
    - total_cost: The total calculated cost.
    """
    # Calculate confusion matrix components
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    # Calculate total cost
    total_cost = (fp * fp_cost) + (fn * fn_cost)

    return total_cost, fp, fn, tp, tn

# Calculate costs for each modelâ€™s predictions
rf_cost, rf_fp, rf_fn, rf_tp, rf_tn = calculate_costs(y_test, y_pred_rf, false_positive_cost, false_negative_cost)
ada_cost, ada_fp, ada_fn, ada_tp, ada_tn = calculate_costs(y_test, y_pred_ada, false_positive_cost, false_negative_cost)
svc_cost, svc_fp, svc_fn, svc_tp, svc_tn = calculate_costs(y_test, y_pred_svc, false_positive_cost, false_negative_cost)
logreg_cost, logreg_fp, logreg_fn, logreg_tp, logreg_tn = calculate_costs(y_test, y_pred_logreg, false_positive_cost, false_negative_cost)

# Display results for each model
print("Cost-Benefit Analysis for Each Model:\n")
print(f"Random Forest - Total Cost: ${rf_cost}")
print(f"  False Positives: {rf_fp}, False Negatives: {rf_fn}, True Positives: {rf_tp}, True Negatives: {rf_tn}\n")

print(f"AdaBoost - Total Cost: ${ada_cost}")
print(f"  False Positives: {ada_fp}, False Negatives: {ada_fn}, True Positives: {ada_tp}, True Negatives: {ada_tn}\n")

print(f"SVC - Total Cost: ${svc_cost}")
print(f"  False Positives: {svc_fp}, False Negatives: {svc_fn}, True Positives: {svc_tp}, True Negatives: {svc_tn}\n")

print(f"Logistic Regression - Total Cost: ${logreg_cost}")
print(f"  False Positives: {logreg_fp}, False Negatives: {logreg_fn}, True Positives: {logreg_tp}, True Negatives: {logreg_tn}\n")

# Summarize costs for easy comparison
model_costs = {
    'Random Forest': rf_cost,
    'AdaBoost': ada_cost,
    'SVC': svc_cost,
    'Logistic Regression': logreg_cost
}

# Find the model with the minimum total cost
best_model = min(model_costs, key=model_costs.get)
print(f"The model with the lowest total cost is: {best_model} with a cost of ${model_costs[best_model]}")

# Model names and their corresponding total costs with visualization
model_names = ['Random Forest', 'AdaBoost', 'SVC', 'Logistic Regression']
total_costs = [1189000, 1445000, 1458000, 1627500]

# Plotting the bar chart for Total Costs
plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, total_costs, color=['#A3C1DA', '#FFB6C1', '#C3E6CB', '#FDD7E4'])  # Pastel colors

# Adding the cost values on top of each bar
for bar, cost in zip(bars, total_costs):
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 50000, f"${cost}", ha='center', va='bottom')

# Labels and Title
plt.xlabel("Model")
plt.ylabel("Total Cost ($)")
plt.title("Cost-Benefit Analysis of Different Models")
plt.ylim(1000000, 1800000)  # Adjust y-axis for better visibility

# Function to plot a single confusion matrix
def plot_confusion_matrix(ax, y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Pastel1', cbar=False, ax=ax,
                xticklabels=['Predicted Low', 'Predicted High'],
                yticklabels=['Actual Low', 'Actual High'])
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    ax.set_title(model_name)

# Plotting confusion matrices side by side
fig, axes = plt.subplots(1, 4, figsize=(20, 5))  # Adjust the figure size if needed

# Assuming y_pred_rf, y_pred_ada, y_pred_svc, y_pred_logreg are predictions for each model
plot_confusion_matrix(axes[0], y_test, y_pred_rf, "Random Forest")
plot_confusion_matrix(axes[1], y_test, y_pred_ada, "AdaBoost")
plot_confusion_matrix(axes[2], y_test, y_pred_svc, "SVC")
plot_confusion_matrix(axes[3], y_test, y_pred_logreg, "Logistic Regression")

plt.tight_layout()
plt.show()

# Initialize UMAP
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)

# Fit and transform the data
X_umap = reducer.fit_transform(X)

#visualization of Test data using PCA, UMAP & T-SNE

# Applying PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_test)

# Applying UMAP for dimensionality reduction
umap_reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = umap_reducer.fit_transform(X_test)

# Applying t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_test)

# Creating DataFrames for plotting
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['Label'] = y_test.values  # Ensure y_test is a Series or a numpy array

umap_df = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])
umap_df['Label'] = y_test.values  # Same as above

tsne_df = pd.DataFrame(X_tsne, columns=['tSNE1', 'tSNE2'])
tsne_df['Label'] = y_test.values  # Same as above

# Side-by-side plots: PCA, UMAP, and t-SNE
fig, ax = plt.subplots(1, 3, figsize=(24, 6))

# PCA Plot
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Label', palette='viridis', s=60, alpha=0.7, ax=ax[0])
ax[0].set_title("PCA of Test Data")
ax[0].set_xlabel("Principal Component 1")
ax[0].set_ylabel("Principal Component 2")
ax[0].legend(title='Label')

# UMAP Plot
sns.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue='Label', palette='viridis', s=60, alpha=0.7, ax=ax[1])
ax[1].set_title("UMAP of Test Data")
ax[1].set_xlabel("UMAP Dimension 1")
ax[1].set_ylabel("UMAP Dimension 2")
ax[1].legend(title='Label')

# t-SNE Plot
sns.scatterplot(data=tsne_df, x='tSNE1', y='tSNE2', hue='Label', palette='viridis', s=60, alpha=0.7, ax=ax[2])
ax[2].set_title("t-SNE of Test Data")
ax[2].set_xlabel("t-SNE Dimension 1")
ax[2].set_ylabel("t-SNE Dimension 2")
ax[2].legend(title='Label')

plt.suptitle("Comparison of PCA, UMAP, and t-SNE on Test Data")
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.model_selection import train_test_split

# Load your data
# Example: Assuming your dataset is in a DataFrame called data_z
# Separate features and target variable
X = data_z.drop(columns=['quality'])
y = data_z['quality']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit PCA on training data
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)

# Fit UMAP on training data
umap_model = umap.UMAP(n_components=2)
X_train_umap = umap_model.fit_transform(X_train)

# Apply t-SNE on training data
tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)
X_train_tsne = tsne.fit_transform(X_train)

# Define soft colors with outlines
colors = ['#ffcccb', '#add8e6']  # Soft red and soft blue
outline_color = 'black'  # Outline color

# Plotting results
plt.figure(figsize=(18, 6))

# PCA Plot
plt.subplot(1, 3, 1)
scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap=plt.cm.colors.ListedColormap(colors),
                      s=50, edgecolor=outline_color)
plt.title('PCA on Training Data')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, ticks=[0, 1], label='Classes')

# UMAP Plot
plt.subplot(1, 3, 2)
scatter = plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_train, cmap=plt.cm.colors.ListedColormap(colors),
                      s=50, edgecolor=outline_color)
plt.title('UMAP on Training Data')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.colorbar(scatter, ticks=[0, 1], label='Classes')

# t-SNE Plot
plt.subplot(1, 3, 3)
scatter = plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=50)
plt.title('t-SNE on Training Data')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(scatter, label='Class Label')

plt.tight_layout()
plt.show()